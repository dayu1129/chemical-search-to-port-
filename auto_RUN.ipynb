### IMPORT LIBRARIES ###
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (TimeoutException,
                                       UnexpectedAlertPresentException,
                                       NoSuchElementException)
import argparse
import pandas as pd
from io import StringIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from bioservices import UniProt
import time
from typing import List, Tuple, Optional

### CONFIGURATION ###
MAX_RETRIES = 3
PAGE_LOAD_TIMEOUT = 30
DRIVER_TIMEOUT = 200
MAX_WORKERS = 50
UNIPROT_RETRIES = 3
UNIPROT_DELAY = 1

### CORE FUNCTIONALITY ###
class TargetCrawler:
    def __init__(self, headless: bool = True):
        self.driver = self._init_driver(headless)
        self.uniprot = UniProt(verbose=False)
        
    def _init_driver(self, headless: bool) -> webdriver.Chrome:
        options = webdriver.ChromeOptions()
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        if headless:
            options.add_argument("--headless=new")
        return webdriver.Chrome(options=options)

    def _handle_exceptions(self, func, *args, **kwargs) -> Optional[pd.DataFrame]:
        retries = 0
        while retries < MAX_RETRIES:
            try:
                return func(*args, **kwargs)
            except (TimeoutException, UnexpectedAlertPresentException) as e:
                print(f"Attempt {retries+1} failed: {str(e)}")
                retries += 1
                time.sleep(2**retries)  # Exponential backoff
            except Exception as e:
                print(f"Critical error: {str(e)}")
                return self._create_error_df(kwargs.get('CpdName', ''), 
                                            kwargs.get('platform', ''), 
                                            str(e))
        return self._create_error_df(kwargs.get('CpdName', ''),
                                      kwargs.get('platform', ''),
                                      f"Max retries ({MAX_RETRIES}) exceeded")

    @staticmethod
    def _create_error_df(compound: str, platform: str, message: str) -> pd.DataFrame:
        return pd.DataFrame({
            'compound': [compound],
            'platform': [platform],
            'UniProt_name': ['error'],
            'prob': [message]
        })

    def _get_uniprot_entry(self, uniprot_id: str) -> str:
        for _ in range(UNIPROT_RETRIES):
            try:
                res = self.uniprot.search(
                    f"{uniprot_id}+AND+organism_id:9606",
                    frmt="tsv",
                    columns="id",
                    limit=1
                )
                if res.count('\n') > 1:
                    return res.split('\n')[1].split('\t')[0]
                return 'no_entry_found'
            except Exception as e:
                print(f"UniProt query failed: {str(e)}")
                time.sleep(UNIPROT_DELAY)
        return 'query_failed'

    def process_uniprot_ids(self, ids: str) -> str:
        if not ids or pd.isna(ids):
            return 'invalid_input'
            
        entries = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = {executor.submit(self._get_uniprot_entry, id_.strip()): id_ 
                      for id_ in ids.split() if id_.strip()}
            
            for future in as_completed(futures):
                try:
                    entries.append(future.result())
                except Exception as e:
                    print(f"UniProt processing error: {str(e)}")
                    entries.append('processing_error')
        
        return '|'.join(filter(None, entries))

### CRAWLER IMPLEMENTATIONS ###
class SEACrawler(TargetCrawler):
    def __call__(self, smiles: str, CpdName: str) -> pd.DataFrame:
        return self._handle_exceptions(self._crawl_sea, smiles, CpdName)

    def _crawl_sea(self, smiles: str, CpdName: str) -> pd.DataFrame:
        self.driver.get('http://sea.bkslab.org/')
        
        WebDriverWait(self.driver, DRIVER_TIMEOUT).until(
            EC.presence_of_element_located((By.NAME, 'query_custom_targets_paste'))
        ).send_keys(smiles + '\n')
        
        # Wait for results
        WebDriverWait(self.driver, DRIVER_TIMEOUT).until(
            EC.presence_of_element_located((By.XPATH, '//table/tbody')))
       
        # Parse table
        html = self.driver.find_element(By.XPATH, '//table').get_attribute('outerHTML')
        df = pd.read_html(StringIO(html))[0]
        
        # Process results
        valid = df['P-Value'].astype(float) < 0.05
        df = df[valid][['Target Key', 'P-Value']]
        df.columns = ['UniProt_name', 'prob']
        
        df.insert(0, 'compound', CpdName)
        df.insert(1, 'platform', 'SEA')
        return df

class SuperPredCrawler(TargetCrawler):
    def __call__(self, smiles: str, CpdName: str) -> pd.DataFrame:
        return self._handle_exceptions(self._crawl_superpred, smiles, CpdName)

    def _crawl_superpred(self, smiles: str, CpdName: str) -> pd.DataFrame:
        self.driver.get('https://prediction.charite.de/subpages/target_prediction.php')
        wait = WebDriverWait(self.driver, 10)
        SearchField =self.driver.find_element(By.XPATH, '//*[@id="smiles_string"]') 
        SearchField.send_keys(smiles) 
        search_button = self.driver.find_elements(By.XPATH, '/html/body/div[2]/div/div/form/div[2]/div/div/button')[0] 
        search_button.click() 
        # Submit SMILES
        startcalculation_button = self.driver.find_element(By.XPATH, '/html/body/div[2]/center/form/table/tbody/tr/td/button') 
        startcalculation_button.click() 
        dfsp = [] 
        max_retries = 3  
        retries = 0 
        all_pages_processed = False
        while retries < max_retries and not all_pages_processed:
            try:
                WebDriverWait(self.driver, 200).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'table')))
                table = WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id="targets"]')))
                table_html = table.get_attribute('outerHTML')
                df = pd.read_html(table_html, header=0)[0]
                cols = [col for col in df.columns if col in ['UniProt ID', 'Probability']]
                df = df[cols]
                df.insert(0, 'compound', CpdName)
                df.insert(1, 'platform', platform)
                df = df.rename(columns={"UniProt ID": "uniprotID", "Probability": "prob"}) 
                dfsp.append(df)  
                ## Check if the "Next" button is available
                try:
                    next_button = self.driver.find_element(By.XPATH, '//*[@id="targets_next"]')
                    if next_button.get_attribute("class") == "paginate_button next disabled":
                        all_pages_processed = True  
                except NoSuchElementException:
                    all_pages_processed = True 
                ## If the "Next" button is available, click the "Next" button to go to the next page
                if not all_pages_processed:
                    next_button.click()
                    WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id="targets"]/tbody')))
                else:
                    break  
            ## Handling exceptional situations: page loading timeout, pop-up warning boxes, and other exceptions.
            except TimeoutException:
                retries += 1            
                if retries >= max_retries:
                    all_pages_processed = True
                    CurrUrl = self.driver.current_url
                    dfsp = pd.DataFrame(columns=['compound', 'platform', 'uniprotID', 'prob'])
                    dfsp = pd.concat([dfsp, pd.DataFrame({'compound': [CpdName],
                                                    'platform': ['superpred'],
                                                    'uniprotID': ['result page reached timeout'],
                                                    'prob': [CurrUrl]})], ignore_index=True)
                    return dfsp
            except UnexpectedAlertPresentException:
                retries += 1            
                if retries >= max_retries:
                    all_pages_processed = True
                    alert = self.driver.switch_to.alert
                    dfsp = pd.DataFrame(columns=['compound', 'platform', 'uniprotID', 'prob'])
                    dfsp = pd.concat([dfsp, pd.DataFrame({'compound': [CpdName],
                                                    'platform': ['superpred'],
                                                    'uniprotID': ['error message'],
                                                    'prob': [alert.text]})], ignore_index=True)
                    alert.accept()
                    return dfsp
            except Exception as e:
                print(f"Error occurred: {e}")
                all_pages_processed = True
                break        
        # 合并所有页面的结果
        final_df = pd.concat(dfsp, ignore_index=True) if dfsp else pd.DataFrame()
        final_df.insert(0, 'compound', CpdName)
        final_df.insert(1, 'platform', 'SuperPred')
        return final_df



### MAIN EXECUTION ###
if __name__ == '__main__':
    import sys
    sys.argv = ['script.py', 
                '-in', 'smile.csv', 
                '-out', 'target_smile_QS.csv',
                '--headless']
    parser = argparse.ArgumentParser(description='Crawl through Target Prediction Servers')     
    parser.add_argument('-in',
        '--input',
        type=str,
        metavar='',
        required=True,
        help='csv-table in the format "name ; smiles-code" of n compounds')    
    parser.add_argument('-out',
        '--output',
        type=str,
        metavar='',
        required=True,
        help='csv-table populated with processed results')
    # 添加headless参数
    parser.add_argument('--headless', 
                        action='store_true',
                        help='Run browser in headless mode (default: True)')
    args = parser.parse_args()
    
    # 初始化爬虫实例
    crawlers = [SEACrawler(headless=args.headless),
                SuperPredCrawler(headless=args.headless)]
    # 处理每个化合物
    results = []
        # 读取输入文件（带错误处理）
    try:
        compounds = pd.read_csv(
            args.input,
            sep=',',  # 明确分隔符
            names=['name', 'smiles'],
            skiprows=1,
            dtype={'name': str, 'smiles': str}  # 强制类型转换
        )
    except Exception as e:
        print(f"读取文件失败: {str(e)}")
        exit(1)

    # 验证数据
    print("成功读取数据样例:")
    print(compounds.head(3))
    # 处理每个化合物
    results = []
    for idx, row in compounds.iterrows():
        try:
            name = str(row['name']).strip()
            smiles = str(row['smiles']).strip()
            current_num = int(idx) + 1  # 显式转换索引为整数
            total = len(compounds)
            print(f"正在处理: {name} ({current_num}/{total})")
            for crawler in crawlers:
                result = crawler(smiles, name)
                results.append(result)
            # 后续处理逻辑...
        except Exception as e:
            print(f"处理第{idx}行时出错: {str(e)}")
            continue
    # Save results
    pd.concat(results).to_csv(args.output, index=False)
    print(f"Results saved to {args.output}")
