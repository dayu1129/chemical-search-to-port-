{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816214c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取数据样例:\n",
      "     name                 smiles\n",
      "0  C4-HSL     CCCC(=O)NC1CCOC1=O\n",
      "1  C6-HSL   CCCCCC(=O)NC1CCOC1=O\n",
      "2  C7-HSL  CCCCCCC(=O)NC1CCOC1=O\n",
      "正在处理: C4-HSL (1/52)\n",
      "正在处理: C6-HSL (2/52)\n",
      "正在处理: C7-HSL (3/52)\n",
      "正在处理: C8-HSL (4/52)\n",
      "正在处理: C9-HSL (5/52)\n",
      "正在处理: C10-HSL (6/52)\n",
      "正在处理: C11-HSL (7/52)\n",
      "正在处理: C12-HSL (8/52)\n",
      "正在处理: C13-HSL (9/52)\n",
      "正在处理: C16-HSL (10/52)\n",
      "正在处理: C16:1-HSL (11/52)\n",
      "正在处理: 3-oxo-C6-HSL (12/52)\n",
      "正在处理: 3-oxo-C9-HSL (13/52)\n",
      "正在处理: 3-oxo-C11-HSL (14/52)\n",
      "正在处理: 3-oxo-C12-HSL (15/52)\n",
      "正在处理: 3-oxo-C13-HSL (16/52)\n",
      "正在处理: 3-OH-C8-HSL (17/52)\n",
      "正在处理: 3-OH-C10-HSL (18/52)\n",
      "正在处理: R-THMF (19/52)\n",
      "正在处理: AI-2 (20/52)\n",
      "正在处理: cyclo(Pro-Phe) (21/52)\n",
      "正在处理: cyclo(Pro-Tyr) (22/52)\n",
      "正在处理: cyclo(Ala-Val) (23/52)\n",
      "正在处理: cyclo(Pro-Leu) (24/52)\n",
      "正在处理: cyclo(L-Pro-L-Tyr) (25/52)\n",
      "正在处理: cyclo(L-Phe-L-Pro) (26/52)\n",
      "正在处理: cyclo(L-Pro-D-Leu) (27/52)\n",
      "正在处理: cyclo(L-Pro-L-Leu) (28/52)\n",
      "正在处理: cyclo(L-Pro-L-Tyr) (29/52)\n",
      "正在处理: cyclo(L-Phe-L-Pro) (30/52)\n",
      "正在处理: cyclo(Gly-L-Phe) (31/52)\n",
      "正在处理: cyclo(D-Pro-L-Tyr) (32/52)\n",
      "正在处理: cyclo(L-Leu-L-Leu) (33/52)\n",
      "正在处理: cyclo(L-Pro-L-Leu) (34/52)\n",
      "正在处理: cyclo(L-Leu-L-Phe) (35/52)\n",
      "正在处理: cyclo(L-Leu-L-Pro) (36/52)\n",
      "正在处理: cyclo(L-Leu-L-Phe) (37/52)\n",
      "正在处理: 2-Octenoic acid (38/52)\n",
      "正在处理: 2-Decenoic acid (39/52)\n",
      "正在处理: 2-Undecenoic acid (40/52)\n",
      "正在处理: 2-Dodecenoic acid (41/52)\n",
      "正在处理: 2-Tridecenoic acid (42/52)\n",
      "正在处理: 11-Methyl-2-dodecenoic acid (43/52)\n",
      "正在处理: 2-Tetradecenoic acid (44/52)\n",
      "正在处理: 2-Pentadecenoic acid (45/52)\n",
      "正在处理: Lauric acid (46/52)\n",
      "正在处理: 11-Methyldodeca-2,5-dienoic acid (47/52)\n",
      "正在处理: cis-11-methyl-2-dodecenoic acid (48/52)\n",
      "正在处理: Trans-11-methyl-2-dodecenoic acid (49/52)\n",
      "正在处理: Decanoic acid (50/52)\n",
      "正在处理: Tridecanoic acid (51/52)\n",
      "正在处理: Pentadecanoic acid (52/52)\n",
      "Results saved to tableS\\S5_target_smile_PA.csv\n"
     ]
    }
   ],
   "source": [
    "### IMPORT LIBRARIES ###\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (TimeoutException,\n",
    "                                       UnexpectedAlertPresentException,\n",
    "                                       NoSuchElementException)\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from bioservices import UniProt\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "### CONFIGURATION ###\n",
    "MAX_RETRIES = 3\n",
    "PAGE_LOAD_TIMEOUT = 30\n",
    "DRIVER_TIMEOUT = 200\n",
    "MAX_WORKERS = 50\n",
    "UNIPROT_RETRIES = 3\n",
    "UNIPROT_DELAY = 1\n",
    "\n",
    "### CORE FUNCTIONALITY ###\n",
    "class TargetCrawler:\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.driver = self._init_driver(headless)\n",
    "        self.uniprot = UniProt(verbose=False)\n",
    "        \n",
    "    def _init_driver(self, headless: bool) -> webdriver.Chrome:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        return webdriver.Chrome(options=options)\n",
    "\n",
    "    def _handle_exceptions(self, func, *args, **kwargs) -> Optional[pd.DataFrame]:\n",
    "        retries = 0\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except (TimeoutException, UnexpectedAlertPresentException) as e:\n",
    "                print(f\"Attempt {retries+1} failed: {str(e)}\")\n",
    "                retries += 1\n",
    "                time.sleep(2**retries)  # Exponential backoff\n",
    "            except Exception as e:\n",
    "                print(f\"Critical error: {str(e)}\")\n",
    "                return self._create_error_df(kwargs.get('CpdName', ''), \n",
    "                                            kwargs.get('platform', ''), \n",
    "                                            str(e))\n",
    "        return self._create_error_df(kwargs.get('CpdName', ''),\n",
    "                                      kwargs.get('platform', ''),\n",
    "                                      f\"Max retries ({MAX_RETRIES}) exceeded\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_error_df(compound: str, platform: str, message: str) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\n",
    "            'compound': [compound],\n",
    "            'platform': [platform],\n",
    "            'UniProt_name': ['error'],\n",
    "            'prob': [message]\n",
    "        })\n",
    "\n",
    "    def _get_uniprot_entry(self, uniprot_id: str) -> str:\n",
    "        for _ in range(UNIPROT_RETRIES):\n",
    "            try:\n",
    "                res = self.uniprot.search(\n",
    "                    f\"{uniprot_id}+AND+organism_id:9606\",\n",
    "                    frmt=\"tsv\",\n",
    "                    columns=\"id\",\n",
    "                    limit=1\n",
    "                )\n",
    "                if res.count('\\n') > 1:\n",
    "                    return res.split('\\n')[1].split('\\t')[0]\n",
    "                return 'no_entry_found'\n",
    "            except Exception as e:\n",
    "                print(f\"UniProt query failed: {str(e)}\")\n",
    "                time.sleep(UNIPROT_DELAY)\n",
    "        return 'query_failed'\n",
    "\n",
    "    def process_uniprot_ids(self, ids: str) -> str:\n",
    "        if not ids or pd.isna(ids):\n",
    "            return 'invalid_input'\n",
    "            \n",
    "        entries = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = {executor.submit(self._get_uniprot_entry, id_.strip()): id_ \n",
    "                      for id_ in ids.split() if id_.strip()}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    entries.append(future.result())\n",
    "                except Exception as e:\n",
    "                    print(f\"UniProt processing error: {str(e)}\")\n",
    "                    entries.append('processing_error')\n",
    "        \n",
    "        return '|'.join(filter(None, entries))\n",
    "\n",
    "### CRAWLER IMPLEMENTATIONS ###\n",
    "class SEACrawler(TargetCrawler):\n",
    "    def __call__(self, smiles: str, CpdName: str) -> pd.DataFrame:\n",
    "        return self._handle_exceptions(self._crawl_sea, smiles, CpdName)\n",
    "\n",
    "    def _crawl_sea(self, smiles: str, CpdName: str) -> pd.DataFrame:\n",
    "        self.driver.get('http://sea.bkslab.org/')\n",
    "        \n",
    "        WebDriverWait(self.driver, DRIVER_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.NAME, 'query_custom_targets_paste'))\n",
    "        ).send_keys(smiles + '\\n')\n",
    "        \n",
    "        # Wait for results\n",
    "        WebDriverWait(self.driver, DRIVER_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//table/tbody')))\n",
    "       \n",
    "        # Parse table\n",
    "        html = self.driver.find_element(By.XPATH, '//table').get_attribute('outerHTML')\n",
    "        df = pd.read_html(StringIO(html))[0]\n",
    "        \n",
    "        # Process results\n",
    "        valid = df['P-Value'].astype(float) < 0.05\n",
    "        df = df[valid][['Target Key', 'P-Value']]\n",
    "        df.columns = ['UniProt_name', 'prob']\n",
    "        \n",
    "        df.insert(0, 'compound', CpdName)\n",
    "        df.insert(1, 'platform', 'SEA')\n",
    "        return df\n",
    "class SuperPredCrawler(TargetCrawler):\n",
    "    def __call__(self, smiles: str, CpdName: str) -> pd.DataFrame:\n",
    "        return self._handle_exceptions(self._crawl_superpred, smiles, CpdName)\n",
    "\n",
    "    def _crawl_superpred(self, smiles: str, CpdName: str) -> pd.DataFrame:\n",
    "        self.driver.get('https://prediction.charite.de/subpages/target_prediction.php')\n",
    "\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, '//*[@id=\"smiles_string\"]').send_keys(smiles)\n",
    "            self.driver.find_elements(By.XPATH, '/html/body/div[2]/div/div/form/div[2]/div/div/button')[0].click()\n",
    "            self.driver.find_element(By.XPATH, '/html/body/div[2]/center/form/table/tbody/tr/td/button').click()\n",
    "        except Exception as e:\n",
    "            return self._error_df(CpdName, \"form submission failed\", str(e))\n",
    "\n",
    "        results = []\n",
    "        retries = 0\n",
    "        max_retries = 3\n",
    "\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 200).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//*[@id=\"targets\"]'))\n",
    "                )\n",
    "\n",
    "                table = self.driver.find_element(By.XPATH, '//*[@id=\"targets\"]')\n",
    "                table_html = table.get_attribute('outerHTML')\n",
    "                df = pd.read_html(StringIO(table_html))[0]\n",
    "                if not {'UniProt_name', 'Probability'}.issubset(df.columns):\n",
    "                    raise ValueError(\"Expected columns not found\")\n",
    "\n",
    "                df = df[['UniProt_name', 'Probability']]\n",
    "                df.insert(0, 'compound', CpdName)\n",
    "                df.insert(1, 'platform', 'superpred')\n",
    "                df.rename(columns={'UniProt_name': 'uniprotID', 'Probability': 'prob'}, inplace=True)\n",
    "                results.append(df)\n",
    "\n",
    "                next_button = self.driver.find_element(By.ID, 'targets_next')\n",
    "                if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                    break\n",
    "                else:\n",
    "                    next_button.click()\n",
    "                    WebDriverWait(self.driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"targets\"]/tbody'))\n",
    "                    )\n",
    "\n",
    "            except TimeoutException:\n",
    "                retries += 1\n",
    "                if retries >= max_retries:\n",
    "                    return self._error_df(CpdName, \"timeout\", self.driver.current_url)\n",
    "\n",
    "            except UnexpectedAlertPresentException:\n",
    "                alert = self.driver.switch_to.alert\n",
    "                alert_text = alert.text\n",
    "                alert.accept()\n",
    "                return self._error_df(CpdName, \"alert\", alert_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                return self._error_df(CpdName, \"unexpected error\", str(e))\n",
    "\n",
    "        return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "    def _error_df(self, CpdName: str, error_type: str, message: str) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\n",
    "            'compound': [CpdName],\n",
    "            'platform': ['superpred'],\n",
    "            'UniProt_name': [error_type],\n",
    "            'prob': [message]\n",
    "        })\n",
    "\n",
    "\n",
    "### MAIN EXECUTION ###\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    sys.argv = ['script.py', \n",
    "                '-in', 'tableS\\S3_compound_info_QS.csv', \n",
    "                '-out', 'tableS\\S5_target_smile_PA.csv',\n",
    "                '--headless']\n",
    "    parser = argparse.ArgumentParser(description='Crawl through Target Prediction Servers')     \n",
    "    parser.add_argument('-in',\n",
    "        '--input',\n",
    "        type=str,\n",
    "        metavar='',\n",
    "        required=True,\n",
    "        help='csv-table in the format \"name ; smiles-code\" of n compounds')    \n",
    "    parser.add_argument('-out',\n",
    "        '--output',\n",
    "        type=str,\n",
    "        metavar='',\n",
    "        required=True,\n",
    "        help='csv-table populated with processed results')\n",
    "    # 添加headless参数\n",
    "    parser.add_argument('--headless', \n",
    "                        action='store_true',\n",
    "                        help='Run browser in headless mode (default: True)')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # 初始化爬虫实例\n",
    "    crawlers = [SEACrawler(headless=args.headless),\n",
    "                SuperPredCrawler(headless=args.headless)]\n",
    "    # 处理每个化合物\n",
    "    results = []\n",
    "        # 读取输入文件（带错误处理）\n",
    "    try:\n",
    "        compounds = pd.read_csv(\n",
    "            args.input,\n",
    "            sep=',',  # 明确分隔符\n",
    "            # # names=['name', 'smiles'],\n",
    "            # skiprows=1,\n",
    "            # dtype={'name': str, 'smiles': str}  # 强制类型转换\n",
    "        )[['name', 'smiles']]\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件失败: {str(e)}\")\n",
    "        exit(1)\n",
    "\n",
    "    # 验证数据\n",
    "    print(\"成功读取数据样例:\")\n",
    "    print(compounds.head(3))\n",
    "    # 处理每个化合物\n",
    "    results = []\n",
    "    for idx, row in compounds.iterrows():\n",
    "        try:\n",
    "            name = str(row['name']).strip()\n",
    "            smiles = str(row['smiles']).strip()\n",
    "            current_num = int(idx) + 1  # 显式转换索引为整数\n",
    "            total = len(compounds)\n",
    "            print(f\"正在处理: {name} ({current_num}/{total})\")\n",
    "            for crawler in crawlers:\n",
    "                result = crawler(smiles, name)\n",
    "                results.append(result)\n",
    "            # 后续处理逻辑...\n",
    "        except Exception as e:\n",
    "            print(f\"处理第{idx}行时出错: {str(e)}\")\n",
    "            continue\n",
    "    # Save results\n",
    "    pd.concat(results).to_csv(args.output, index=False)\n",
    "    print(f\"Results saved to {args.output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
